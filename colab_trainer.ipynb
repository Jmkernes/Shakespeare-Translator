{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare Translator Colab Training Notebook\n",
    "\n",
    "This is an interactive version of the training script. It make it easier to tweak, analyze, and view the model in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run below if in colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import DataManager, load_sentencepiece_model\n",
    "from utils import print_bar, predict, create_masks\n",
    "from model import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train utility functions. Custom loss, accuracy, and schedule\n",
    "\n",
    "The custom learning rate schedule will increase linearly to about 1e-4, then decay ~1/sqrt(step) thereafter.\n",
    "\n",
    "The model doesn't do well if the learning rate gets as high as 1e-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom learning rate schedulers\n",
    "class RootDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, max_lr, warmup_steps, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_lr = max_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "    def __call__(self, step):\n",
    "        linear = self.max_lr*(step/self.warmup_steps)\n",
    "        decay_steps = tf.math.maximum(step-self.warmup_steps, 1e-7)\n",
    "        fall = self.max_lr*0.1**(decay_steps/5000.)\n",
    "        return tf.math.minimum(linear, fall)\n",
    "    \n",
    "class TransformerSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"warmup_steps\": self.warmup_steps}\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)/ 10.\n",
    "    \n",
    "def loss_fn(labels, logits):\n",
    "    mask = tf.not_equal(labels, 0)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)\n",
    "    mask = tf.cast(mask, loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "\n",
    "def accuracy_fn(labels, logits):\n",
    "    mask = tf.not_equal(labels, 0)\n",
    "    preds = tf.cast(tf.argmax(logits, axis=-1), labels.dtype)\n",
    "    acc = tf.cast(tf.equal(labels, preds), tf.float32)\n",
    "    acc = tf.boolean_mask(acc, mask)\n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer configurations\n",
    "\n",
    "SentencePiece has the option to randomize the tokenization. This means we can ask it to randomly choose from the top \"nbest_size\" number of tokenization sequences. This helps to both regularize the model and make it more robust to learning a variety of subword meanings. This is a more of data augmentation and greatly enhances the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text='data_clean/modern.txt'\n",
    "target_text='data_clean/original.txt'\n",
    "inp_sp_model_file='tokenizers/modern2k.model'\n",
    "tar_sp_model_file='tokenizers/original2k.model'\n",
    "inp_nbest_size = 5\n",
    "tar_nbest_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text data\n",
    "\n",
    "SentencePiece is blazingly fast, so we train directly on the text data (which also lets us do the tokenization sampling). The caveat, is the input data needs to have been minorly cleaned, which it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    'input_text':input_text,\n",
    "    'target_text':target_text,\n",
    "    'inp_sp_model_file':inp_sp_model_file,\n",
    "    'tar_sp_model_file':tar_sp_model_file,\n",
    "    'inp_nbest_size':inp_nbest_size,\n",
    "    'tar_nbest_size':tar_nbest_size\n",
    "}\n",
    "dm = DataManager.directly_from_text(data_config)\n",
    "input_vocab_size = int(dm.inp_tokenizer.vocab_size())\n",
    "target_vocab_size = int(dm.tar_tokenizer.vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Initializing model...\n",
      "Model parameters:\n",
      "{'num_layers': 4, 'd_model': 512, 'num_heads': 4, 'd_ffn': 1024, 'input_vocab_size': 2048, 'target_vocab_size': 2048, 'pe_input': 512, 'pe_target': 512, 'p_drop': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Setup the model\n",
    "num_layers = 4\n",
    "d_model=512\n",
    "num_heads=4\n",
    "d_ffn=1024\n",
    "max_position=512\n",
    "dropout_rate=0.1\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "config = {\n",
    "    'num_layers': num_layers,\n",
    "    'd_model':d_model,\n",
    "    'num_heads': num_heads,\n",
    "    'd_ffn': d_ffn,\n",
    "    'input_vocab_size': input_vocab_size,\n",
    "    'target_vocab_size': target_vocab_size,\n",
    "    'pe_input': max_position,\n",
    "    'pe_target': max_position,\n",
    "    'p_drop': dropout_rate\n",
    "}\n",
    "\n",
    "model = Transformer(**config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set optimizer and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = 1000\n",
    "learning_rate = TransformerSchedule(d_model=d_model, warmup_steps=warmup_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metrics and training and evaluation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "valid_loss = tf.keras.metrics.Mean()\n",
    "train_acc = tf.keras.metrics.Mean()\n",
    "valid_acc = tf.keras.metrics.Mean()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_in = tar[:, :-1]\n",
    "    tar_out = tar[:, 1:]\n",
    "    tar_len = tf.shape(tar_in)[1]\n",
    "    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar_in)\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, _ = model(inp, tar_in, training=True, enc_padding_mask=enc_padding_mask,\n",
    "                       look_ahead_mask=look_ahead_mask, dec_padding_mask=dec_padding_mask)\n",
    "        loss = loss_fn(tar_out, logits)\n",
    "    accuracy = accuracy_fn(tar_out, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_acc(accuracy)\n",
    "\n",
    "@tf.function\n",
    "def evaluation_step(inp, tar):\n",
    "    tar_in = tar[:, :-1]\n",
    "    tar_out = tar[:, 1:]\n",
    "    tar_len = tf.shape(tar_in)[1]\n",
    "    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar_in)\n",
    "    logits, _ = model(inp, tar_in, training=False, enc_padding_mask=enc_padding_mask,\n",
    "                   look_ahead_mask=look_ahead_mask, dec_padding_mask=dec_padding_mask)\n",
    "    loss = loss_fn(tar_out, logits)\n",
    "    accuracy = accuracy_fn(tar_out, logits)\n",
    "    valid_loss(loss)\n",
    "    valid_acc(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize TensorBoard logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r logs\n",
    "train_log_dir = './logs/' + '/train'\n",
    "test_log_dir = './logs/' + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set max input sequences length, initialize global step, initialize train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=100\n",
    "\n",
    "# Configure datasets for training\n",
    "glob_step = tf.Variable(0, dtype=tf.int64) # This will break tf.summary if we use int32\n",
    "train_ds, valid_ds = dm.get_train_valid_datasets(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [optional] Print out a model summary\n",
    "\n",
    "We need to input some data into the model so that it can initialize it's weights. Afterwards we can print out the model summary. This makes the code awkward, as it's right in the middle of the dataset setup. This is because we want to do this before caching the dataset, but we can't do it until we have initialized the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_inp, temp_tar = next(iter(train_ds.batch(1)))\n",
    "model(temp_inp, temp_tar[:, :-1], False, None, None, None)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure dataset for training: shuffle, batch, cache, prefetch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "valid_ds = valid_ds.cache().batch(batch_size, drop_remainder=True)\n",
    "valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)\n",
    "DATASET_SIZE = int(dm.ds_size//batch_size)\n",
    "iterator=iter(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "For each loop we run a training step and print out the loss and accuracy to a log or stdout. If run in colab, the print bar doesn't show up for some reason, so we manually print every 100 steps. On epoch ends, we evaluate the validation set and log the information. \n",
    "\n",
    "Every 500 steps we also print out the current model's prediction of a simple sentence \"Where are you?\". Given our limited HS english, we expect this to be something long the lines of \"Wherefore art thou?\"\n",
    "\n",
    "The log information should hopefully appear in the tensorboard app a few cells back. We must always re-run that tensorboard cell before we run the training loop cell. If you have to restart the training loop cell, make sure you remove the ```!rm -r logs``` line in the tensorboard cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "\n",
    "absolute_start = time.time()\n",
    "print(\"\\n\\n~~~~~~~~~~ Beginning training ~~~~~~~~~~\")\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print('\\n'+'-'*10+f' Epoch {epoch+1}/{epochs} '+'-'*10)\n",
    "    start = time.time()\n",
    "    for metric in [train_loss, valid_loss, train_acc, valid_acc]:\n",
    "        metric.reset_states()\n",
    "\n",
    "    for step, (inp, tar) in enumerate(train_ds):\n",
    "        \n",
    "        train_step(inp, tar)\n",
    "        \n",
    "        diff = (time.time()-start)/(step+1)\n",
    "        print_bar(step, DATASET_SIZE, diff, train_loss.result().numpy())\n",
    "        if (int(glob_step)+1)%100==0:\n",
    "            step = int(glob_step)\n",
    "            iter_message = f\"Iteration {step+1:02d}/{DATASET_SIZE*epochs}:\"\n",
    "            time_message = f\" {1/diff:.2f} it/s.\"\n",
    "            loss_message = f\" Loss: {float(train_loss.result()):.3f}\"\n",
    "            acc_message = f\" Accuracy: {float(train_acc.result()):.3f}\"\n",
    "            print(iter_message+time_message+loss_message+acc_message)\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=glob_step)\n",
    "            tf.summary.scalar('accuracy', train_acc.result(), step=glob_step)\n",
    "            tf.summary.scalar('lr', learning_rate(tf.cast(glob_step, tf.float32)), step=glob_step)\n",
    "        glob_step.assign_add(1)\n",
    "\n",
    "        if int(glob_step)%500==0:\n",
    "            sentence = 'where are you?'\n",
    "            pred_sentence = dm.tar_tokenizer.detokenize(predict(sentence, dm.inp_tokenizer, model)[0])\n",
    "            print(f\"Input sentence: {sentence}\")\n",
    "            print(f\"Output sentence: {pred_sentence.numpy()[0].decode()}\")\n",
    "\n",
    "    if DATASET_SIZE is None:\n",
    "        DATASET_SIZE = int(glob_step)\n",
    "\n",
    "    for inp, tar in valid_ds:\n",
    "        evaluation_step(inp, tar)\n",
    "\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', valid_loss.result(), step=glob_step)\n",
    "        tf.summary.scalar('accuracy', valid_acc.result(), step=glob_step)\n",
    "\n",
    "tot_time = time.time()-absolute_start\n",
    "minutes = int(tot_time)//60\n",
    "seconds = int(tot_time)%60\n",
    "print('*'*100+\"\\n\\nTRAINING COMPLETE.\\n\\n\"+'*'*100)\n",
    "print(f\"\\n\\nTotal time: {minutes:02d}min. {seconds:02d}sec.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "Since we are running this in a jupyter notebook, we don't have any checkpoint files. This is assuming your browswer won't crash and you can stop training and save at any time.\n",
    "\n",
    "If you want the checkpoints, you can copy them over from the script code. Or just run the script!\n",
    "\n",
    "We need to save 1) the tensorboard logs 2) the model configuration (so we can reload it from the weights) 3) the model weights\n",
    "\n",
    "Trying to directly save the model will fail, as it doesn't like that model takes multiple inputs (inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask). I haven't figured out how to deal with that, other than to just save the weights and make a new model used only for inference, that doesn't take any tar or mask input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "!mv logs translate_logs\n",
    "!zip -r translate_logs.zip translate_logs\n",
    "files.download('translate_logs.zip')\n",
    "\n",
    "with open('model_config.json', 'w') as file:\n",
    "    file.write(json.dumps(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('translator_weights', )\n",
    "files.download('/content/translator_weights.index')\n",
    "files.download('/content/translator_weights.data-00000-of-00001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from model import Transformer\n",
    "from utils import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('config.json'):\n",
    "    with open('config.json', 'r') as file:\n",
    "        config = json.loads(file.read())\n",
    "else:\n",
    "    raise FileExistsError('Could not find configuration file.')\n",
    "        \n",
    "model = Transformer(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fefd3c1d070>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_inp = tf.random.uniform((1,4), 0, 10, tf.int32)\n",
    "_ = model(random_inp, random_inp, training=False, enc_padding_mask=None, dec_padding_mask=None, look_ahead_mask=None)\n",
    "\n",
    "model.load_weights('translator_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tf.Tensor(b'the king hath funed, but in his mind died.', shape=(), dtype=string)\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('The king fought hard, but in the end he died.', inp_tokenizer, tar_tokenizer, model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3\n",
    "# DEEP LEARNING AMI\n",
    "# p2 xlarge is the smallest machine learning\n",
    "# have to be in the .ssh folder when ssh'ing. use ssh -i enter_thing_here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
